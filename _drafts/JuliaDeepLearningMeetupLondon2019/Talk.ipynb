{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow and other tools for ML in Julia\n",
    "\n",
    "**Lyndon White**\n",
    " - Research Software Engineer -- Invenia Labs, Cambridge\n",
    " - Technically still PhD Candidate -- The University of Western Australia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  99.2 %0.0 %>]  100.0 %\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/Invenia`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `git@gitlab.invenia.ca:invenia/PackageRegistry.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m WeakRefStrings â”€ v0.5.6\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Manifest.toml`\n",
      " \u001b[90m [a93c6f00]\u001b[39m\u001b[93m â†‘ DataFrames v0.17.0 â‡’ v0.17.1\u001b[39m\n",
      " \u001b[90m [4c63d2b9]\u001b[39m\u001b[93m â†‘ StatsFuns v0.7.1 â‡’ v0.8.0\u001b[39m\n",
      " \u001b[90m [bd369af6]\u001b[39m\u001b[93m â†‘ Tables v0.1.14 â‡’ v0.1.15\u001b[39m\n",
      " \u001b[90m [ea10d353]\u001b[39m\u001b[93m â†‘ WeakRefStrings v0.5.4 â‡’ v0.5.6\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg: @pkg_str\n",
    "pkg\"activate  .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”Œ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/MLDataUtils/CQWB9.ji for MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d]\n",
      "â”” @ Base loading.jl:1184\n",
      "â”Œ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "â”‚ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "â”” @ Base loading.jl:947\n",
      "â”Œ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "â”‚ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "â”” @ Base loading.jl:947\n",
      "â”Œ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "â”” @ Base loading.jl:1186\n",
      "â”Œ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "â”‚ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "â”” @ Base loading.jl:947\n",
      "â”Œ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/DataStreams/E9VAJ.ji for DataStreams [9a8bc11e-79be-5b39-94d7-1ccc349a1a85]\n",
      "â”” @ Base loading.jl:1184\n",
      "â”Œ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/Tables/Z804B.ji for Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c]\n",
      "â”” @ Base loading.jl:1184\n"
     ]
    },
    {
     "ename": "CompositeException",
     "evalue": "On worker 2:\nArgumentError: Package MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\n_require at ./loading.jl:929\nrequire at ./loading.jl:858\n#2 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/Distributed.jl:77\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259\n#remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421\nremotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412\n#remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\nremotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\n(::getfield(Distributed, Symbol(\"##1#3\")){Base.PkgId})() at ./task.jl:259",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nArgumentError: Package MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\n_require at ./loading.jl:929\nrequire at ./loading.jl:858\n#2 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/Distributed.jl:77\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259\n#remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421\nremotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412\n#remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\nremotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\n(::getfield(Distributed, Symbol(\"##1#3\")){Base.PkgId})() at ./task.jl:259",
      "",
      "Stacktrace:",
      " [1] sync_end(::Array{Any,1}) at ./task.jl:226",
      " [2] _require_callback(::Base.PkgId) at ./task.jl:245",
      " [3] #invokelatest#1 at ./essentials.jl:742 [inlined]",
      " [4] invokelatest at ./essentials.jl:741 [inlined]",
      " [5] require(::Base.PkgId) at ./loading.jl:861",
      " [6] require(::Module, ::Symbol) at ./loading.jl:853",
      " [7] top-level scope at In[8]:3"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Plots\n",
    "using MLDataUtils\n",
    "\n",
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before TensorFlow\n",
    " \n",
    " - Researchers generally couldn't use Cafe etc as it is not suitably flexible.\n",
    " - One could use Theano, but it is painfully weird\n",
    " - Just write your neural networks by hand, as matrix math\n",
    " - And do your differenciation by hand, first with a blackboard then with more matrix math\n",
    " - While you are at it maybe write your own implementation of gradient descent etc\n",
    " - This was not long ago, I was still doing this in 2014\n",
    " - Julia is a fantasitic language to do this in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Static Graphs\n",
    "\n",
    " - Easy to manipulate mathematically and easy to think about\n",
    "     - It is literally an AST for a language without control flow\n",
    "        - i.e.  a language that is a lot like mathematical notation  \n",
    "     - The dervitive of the graph can be calculated  via the chain rule -- generating another graph\n",
    " - Dynamic stuctures are impossible\n",
    "     - A dynamic structure is on in which the network structure differs per input\n",
    "     - RNNs have to be statically unrolled to their maximum length\n",
    "     - If you want to represent say a tree structured network  (e.g. the work of Bowman, Socher and others for NLP)...  **Cry**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Types of Nodes, i.e. `Tensors`\n",
    " - **Placeholders:** this is where you put your inputs\n",
    " - **Operations:** theres transform inputs into outputs, they do math\n",
    " - **Variables:** thes arre the things you train, they are mutable\n",
    " - **Actions:** These are operations with side effects, like logging (TensorBoard)) and mutating Variable (Optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions\n",
    "\n",
    "Functions mutate **the graph** to introduce nodes.\n",
    "\n",
    "For example:\n",
    " - `sin(::Float64)` in julia would return a `Float64` that is the answer.\n",
    " - `sin(::Tensor)` introduces a `sin` operation into the graph, and returns a `Tensor` that is a reference to it's output, this could be feed to other operations.\n",
    " \n",
    "The answer to that operation is not computed, until you execute the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-06 10:43:35.595159: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”Œ Warning: Your Python TensorFlow client version (1.10.0) is below the TensorFlow backend version (1.12.0). This can cause various errors. Please upgrade your Python TensorFlow installation and then restart Julia.\n",
      "â”‚ You can upgrade by calling `using Conda; Conda.update();` from Julia.\n",
      "â”” @ TensorFlow /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/version.jl:57\n"
     ]
    }
   ],
   "source": [
    "# Create a summary writer\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "x_summary = TensorFlow.summary.scalar(\"x\", x)\n",
    "y_summary = TensorFlow.summary.scalar(\"y\", y)\n",
    "\n",
    "merged_summary_op = TensorFlow.summary.merge_all()\n",
    "\n",
    "for (ii, x_val) in enumerate(-1:0.1:1)\n",
    "    y_val, summaries = run(sess, [y, merged_summary_op], Dict(x=>x_val))\n",
    "    write(summary_writer, summaries,  ii)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "close(summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = TensorFlow.tensorflow.Event()\n",
    "data=convert(Vector{UInt8}, collect(\"\\n,\\n\\x01x\\x12\\vPlaceholder*\\v\\n\\x05dtype\\x12\\x020\\x02*\\r\\n\\x05shape\\x12\\x04:\\x02\\x18\\x01\\n\\x14\\n\\x01y\\x12\\x03Sin\\x1a\\x01x*\\a\\n\\x01T\\x12\\x020\\x02\\x12\\0\\\"\\x02\\b\\x1a\"))\n",
    "setfield!(event, :graph_def, data)\n",
    "write(summary_writer, event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "logdir = \"logs\"\n",
    "mkpath(logdir)\n",
    "\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(logdir; graph=sess.graph)\n",
    "close(summary_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Node Naming\n",
    "\n",
    " - Notice before I did `@tf begin ... end`\n",
    " - **This is not at all required**\n",
    " - But it does enable automatic node naming\n",
    " - so `@tf y = sin(x)` actually becomes `y = sin(x; name=\"y\")`\n",
    " - This gives you a good graph in tensorboard, and also better error messages.\n",
    " - Further it lets us look up tensors from the graph by **name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.graph[\"x\"] = <Tensor x:1 shape=unknown dtype=Float64>\n",
      "sess.graph[\"y\"] = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show sess.graph[\"x\"]\n",
    "@show sess.graph[\"y\"]\n",
    "\n",
    "run(sess, sess.graph[\"y\"], Dict(sess.graph[\"x\"]=>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets have an exciting Demo\n",
    "\n",
    "![](https://white.ucc.asn.au/posts_assets/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_files/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_28_0.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "RemoteException",
     "evalue": "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
      "",
      "Stacktrace:",
      " [1] #remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421",
      " [2] remotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412",
      " [3] #remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [4] remotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [5] top-level scope at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:189",
      " [6] eval at ./boot.jl:328 [inlined]",
      " [7] eval at ./sysimg.jl:68 [inlined]",
      " [8] add_gradients_py(::Tensor{Float64}, ::Array{Any,1}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1548",
      " [9] gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1536 [inlined] (repeats 2 times)",
      " [10] compute_gradients(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:49",
      " [11] #minimize#1(::Nothing, ::Nothing, ::Nothing, ::Function, ::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:41",
      " [12] minimize(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:38",
      " [13] top-level scope at In[9]:31"
     ]
    }
   ],
   "source": [
    "sess = Session(Graph())\n",
    "\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "\n",
    "# Network Definition\n",
    "@tf begin\n",
    "    X = placeholder(Float32, shape=[-1, 28*28])\n",
    "    \n",
    "    # Network parameters\n",
    "    hl_sizes = [512, 128, 64, 2, 64, 128, 512]\n",
    "\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "    \n",
    "    Wout = get_variable([get_shape(Zs[end], 2), 28*28], Float32)\n",
    "    bout = get_variable([28*28], Float32)\n",
    "    Y = nn.sigmoid(Zs[end]*Wout + bout)\n",
    "    \n",
    "    \n",
    "    Z_code = Zs[endÃ·2 + 1] # A name for the coding layer\n",
    "    @assert get_shape(Z_code,2) == 2\n",
    "end\n",
    "\n",
    "losses = 0.5(Y .- X).^2\n",
    "loss = reduce_mean(losses)\n",
    "optimizer = train.minimize(train.AdamOptimizer(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = MNIST.traintensor()\n",
    "test_images = MNIST.testtensor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter_image (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function one_image(img::Vector, frames_image_res=30)\n",
    "    ret = zeros((frames_image_res, frames_image_res))\n",
    "    ret[2:end-1, 2:end-1] = 1 .- rotl90(reshape(img, (28,28)))\n",
    "    ret\n",
    "end\n",
    "\n",
    "function scatter_image(images, res; frames_image_res=30, no_overlap=false)\n",
    "    canvas = ones(res, res)\n",
    "    images = reshape(images, (28*28, :));\n",
    "    codes = run(sess, Z_code, Dict(X=>images'))\n",
    "    for ii in 1:2\n",
    "        codes[:,ii] = (codes[:,ii] .- minimum(codes[:,ii]))./(maximum(codes[:,ii])-minimum(codes[:,ii]))\n",
    "        @assert(minimum(codes[:,ii]) >= 0.0)\n",
    "        @assert(maximum(codes[:,ii]) <= 1.0)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    function target_area(code)\n",
    "        central_res = res-frames_image_res-1\n",
    "        border_offset = frames_image_res/2 + 1\n",
    "        x,y = code*central_res .+ border_offset\n",
    "        \n",
    "        get_pos(v) = round(Int, v-frames_image_res/2)\n",
    "        x_min = get_pos(x)\n",
    "        x_max = x_min + frames_image_res-1\n",
    "        y_min =  get_pos(y)\n",
    "        y_max = y_min + frames_image_res-1\n",
    "        \n",
    "        @view canvas[x_min:x_max, y_min:y_max]\n",
    "    end\n",
    "    \n",
    "    for ii in 1:size(codes, 1)\n",
    "        code = codes[ii,:]\n",
    "        img = images[:,ii]\n",
    "        area = target_area(code)        \n",
    "        no_overlap && any(area.<1) && continue # Don't draw over anything\n",
    "        area[:] = one_image(img, frames_image_res)\n",
    "    end\n",
    "    canvas\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: ObsDim not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ObsDim not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at ./In[12]:5"
     ]
    }
   ],
   "source": [
    "#run(sess, global_variables_initializer())\n",
    "auto_loss = Float64[]\n",
    "for epoch in 1:20\n",
    "    epoch_loss = Float64[]\n",
    "    for (batch_ii, batch_x) in  enumerate(eachbatch(train_images, 1_000, ObsDim.Last()))\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "        \n",
    "        if ii % 5 == 1 \n",
    "            println(\"Batch $batch_ii loss: $(loss_o)\")\n",
    "            display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "            IJulia.clear_output(true)\n",
    "        end\n",
    "    end\n",
    "    push!(auto_loss, mean(epoch_loss))\n",
    "    \n",
    "    #\n",
    "#    println(\"Epoch $epoch loss: $(auto_loss[end])\")\n",
    "#    display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "#    IJulia.clear_output(true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets break that example down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining a Custom Activation Function\n",
    "```\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "```\n",
    "\n",
    " - Trival in the modern day with Flux, etc\n",
    " - When TensorFlow came out, this was insane wizard tricks, for Cafe users.\n",
    " - But now we take it for granted.\n",
    " - Note that to do this TensorFlow needed to basically implement a full linear algebra and math library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building up our layers\n",
    "\n",
    "```\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "```\n",
    "\n",
    "Remember what we are actually doing here is mutating the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLDataUtils for training helpers\n",
    "\n",
    "```\n",
    "    for batch_x in eachbatch(train_images, 1_000, ObsDim.Last())\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "    end\n",
    "```\n",
    "\n",
    " - MLDataUtils is a fantastic julia package full of helpers useful with all ML packages\n",
    " - Use it with TensorFlow, use it with Flux, use it with Knet\n",
    " - `eachbatch`/ `batchview`\n",
    " - `eachobs`/`obsview`\n",
    " - Various stratified sampling, `oversample`, `undersample`\n",
    " - test/train splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Complicated Output Layer for HSV Color\n",
    " - Saturation and Value are easy, but Hue is angular\n",
    "$$\n",
    "loss =\n",
    "\\frac{1}{2} \\left(\\sin(y^\\star_{hue}) - y_{shue} \\right)^2     \n",
    "+ \\frac{1}{2} \\left(\\cos(y^\\star_{hue}) - y_{chue} \\right)^2  \n",
    "+ \\left(y^\\star_{sat} - y_{sat} \\right)^2  \n",
    "+ \\left(y^\\star_{val} - y_{val} \\right)^2 %\n",
    "$$\n",
    " \n",
    "<img src=\"./figs/hsv_output_module.png\" width=\"50%\" height=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we build this\n",
    "## (Syntax Overloading)\n",
    "\n",
    "\n",
    "```julia\n",
    "function hsv_output_layer(Y_logit::Tensor{Float32})\n",
    "    # Y_logit size: [missing, 4] \n",
    "\n",
    "    \n",
    "    # Prediction \n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "\n",
    "\n",
    "    # Obs \n",
    "    Y_obs = placeholder(Float32; shape=[-1, 3])\n",
    "    Y_obs_hue = Y_obs[:,1]                       # Notice proper indexing         \n",
    "    Y_obs_sat = Y_obs[:,2]\n",
    "    Y_obs_val = Y_obs[:,3]\n",
    "\n",
    "    Y_obs_shue = sin(Float32(2Ï€) .* Y_obs_hue)\n",
    "    Y_obs_chue = cos(Float32(2Ï€) .* Y_obs_hue)\n",
    "    \n",
    "    \n",
    "    # Loss                        \n",
    "    loss_hue = 0.5reduce_mean((Y_shue - Y_obs_shue)^2 + (Y_chue - Y_obs_chue)^2))\n",
    "    loss_sat = reduce_mean((Y_sat-Y_obs_sat)^2)\n",
    "    loss_val = reduce_mean((Y_val-Y_obs_val)^2)\n",
    "\n",
    "    loss_total = identity(loss_hue + loss_sat + loss_val)\n",
    "\n",
    "                        \n",
    "    # For Output, we want hue angle measured in 0:1 (units of turns)\n",
    "    Y_hue_o1 = Ops.atan2(Y_shue, Y_chue)/(2Float32(Ï€))\n",
    "    Y_hue_o2 = select(Y_hue_o1 > 0, Y_hue_o1, Y_hue_o1+1) # Wrap around things below 0\n",
    "    Y_hue = reshape(Y_hue_o2, [-1]) # force shape\n",
    "\n",
    "    Y = identity([Y_hue Y_sat Y_val]) # *** Notice Julia Style hcat***\n",
    "\n",
    "    return loss_total\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overloading hcat & vcat\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    " Y = identity([Y_hue Y_sat Y_val])\n",
    "```\n",
    "\n",
    "\n",
    "So that `[a b]` and `[a; b]` work.\n",
    "vs Base Tensorflow, would have you first make sure everything is the same number of dimensions,\n",
    "then `concat` them,\n",
    "And you couldn't use julia style syntax.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/7099f05f523556829164aab41eccd394d29df898/src/ops/transformations.jl#L129-L150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overloading getindex\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "```\n",
    "\n",
    "Indexing with slices and ranges is much nicer than `tf.gather` and `tf.gather_nd` and even than `tf.slice`.\n",
    "\n",
    "So that `X[a:b]`, `X[a]`, `X[:, endÃ·2]` etc.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/master/src/ops/indexing.jl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow.jl Conventions vs Julia Conventions vs Python TensorFlow Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Julia**: 1-based indexing   \n",
    "**Python TF**: 0-based indexing  \n",
    "**TensorFlow.jl**: 1-based indexing   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:** explicit broadcasting   \n",
    "**Python TF:** implicit broadcasting   \n",
    "**TensorFlow.jl:** implicit or explicit broadcasting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:**  last index at `end`, 2nd last in `end-1`, etc.   \n",
    "**Python TF:** last index at `-1` second last in `-2`   \n",
    "**TensorFlow.jl** last index at `end` 2nd last in `end-1`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:**  Operations in Julia ecosystem namespaces. (`SVD` in `LinearAlgebra`, `erfc` in `SpecialFunctions`, `cos` in `Base`)   \n",
    "**Python TF:** All operations in TensorFlow's namespaces (`SVD` in `tf.linalg`, `erfc` in `tf.math`, `cos` in `tf.math`, and all reexported from `tf`)  \n",
    "**TensorFlow.jl**  Existing Julia functions overloaded to call TensorFlow equivalents when called with TensorFlow arguments  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:** Container types are parametrized by number of dimensions and element type   \n",
    "**Python TF:** N/A -- python does not have a parametric type system   \n",
    "**TensorFlow.jl:** Tensors are parametrized by element type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are the bits that make TensorFlow.jl work defined?\n",
    "\n",
    "## TensorFlow.jl (Julia)\n",
    " - Nice Things\n",
    " - RNNs\n",
    " - Training / Optimizers\n",
    " \n",
    "## TensorFlow (PyCall)\n",
    " - Gradients\n",
    " - Writing tensorboard events to file\n",
    " \n",
    "## LibTensorFlow (C API)\n",
    " - Operations\n",
    " - Shape Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ðŸ˜¢ BatchNorm\n",
    "\n",
    " - There is a `BatchNorm` op in LibTensorFlow\n",
    " - Actually there are several, for different parts of the Fusing.\n",
    " - to get `BatchNorm` to work, you need to glue these together with the right predeclared variable for state and for reused working memory\n",
    " - This is hundreds (thousands?) of lines of python glue code, that needs to be reimplemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  ðŸ˜¢ Windows Support\n",
    "\n",
    " - I've not tried to get this working in  a while but last time:\n",
    " - Unending segfaults on basic operations.\n",
    " - In theory it should just work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  TFEager\n",
    "## Work In Progress\n",
    "\n",
    " - Jon Malmaud is working on this\n",
    " - Google apparently wants this.\n",
    " - But why? I have a perfectly nice eager NN framework called Flux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropping the Python Dependency\n",
    " - Python dependency is a nasty hack\n",
    " - It is basically only used for getting gradients.\n",
    " - we actually interact with it primarily by:\n",
    "     - exporting the graph\n",
    "     - running some Python TF on it\n",
    "     - Importing the modified graph back\n",
    "     \n",
    " - We need it for gradients as they are not in the C API\n",
    " - They are coming to the C API, but not ready yet.]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Invenia Labs\n",
    "\n",
    "![](https://www.invenia.ca/wp-content/themes/relish_theme/img/labs-logo.png)\n",
    "\n",
    "## We're hiring\n",
    "### People who know Julia\n",
    "### People who know Machine Learning\n",
    "I have left some fliers about open positions at the entrance."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
