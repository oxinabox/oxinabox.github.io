{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow and other tools for ML in Julia\n",
    "\n",
    "\n",
    "![](data:image/svg+xml;base64,<svg viewBox="0 0 2575 495" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><g fill-rule="nonzero"><path d="M199.243 264.837l66.414-38.537 65.594 38.537-65.594 37.716-66.414-36.896v-.82z" fill="#f6bd3a"/><path d="M0 150.867L265.657 0l197.602 112.33-65.594 113.97-132.008-75.433-199.243 113.15L0 150.867z" fill="#f6bd3a"/><path d="M199.243 415.703V340.27l65.594-37.717 66.414-37.716v75.433l-65.594 37.717v75.433l-66.414 38.537v-76.254zm0-150.866l-66.415-37.717-66.414 36.897v-75.434l199.243-113.15V226.3l-66.414 38.537zm197.602-76.254v-37.716l66.414-38.537.82 75.434-66.414 38.536-.82-37.717z" fill="#eb8c23"/><path d="M132.828 451.78V227.12l65.595-36.897.82 74.614 66.414 36.897v76.253l-66.414-35.257v149.227l-66.415-40.177zM32.798 244.34L0 224.66v-73.793l66.414 37.716v75.434l-33.617-19.678zm232.859-93.472V75.433l131.188 75.434.82 75.433-132.008-75.433z" fill="#e35a2b"/></g><path d="M636.747 169.726v282.83h-49.746v-282.83h-96.646v-48.797h243.038v48.797h-96.646z" fill="#ef6639" fill-rule="nonzero"/><path d="M898.006 272.54c4.263 15.16 4.35 35.115 4.036 37.643l-160.56 47.6c1.264 12.003.896 7.137 4.685 15.665 3.79 8.528 8.528 15.554 14.213 21.08 5.686 5.528 12 9.637 18.95 12.318a59.952 59.952 0 0 0 21.795 4.028c10.421 0 19.975-1.738 28.66-5.21 8.686-3.474 17.135-8.372 25.346-14.686l28.9 31.267c-11.053 10.424-23.374 18.479-36.955 24.161-13.579 5.686-29.844 8.528-48.797 8.528-14.528 0-28.344-2.842-41.453-8.528-13.108-5.685-24.555-14.055-34.347-25.11-9.79-11.054-17.53-24.633-23.212-40.743-5.686-16.106-8.531-34.424-8.531-54.955 0-21.16 2.687-39.795 8.054-55.901 5.371-16.11 12.794-29.612 22.269-40.507 9.475-10.897 20.922-19.187 34.347-24.873 13.423-5.685 28.187-8.528 44.296-8.528 17.686 0 33.084 3.316 46.19 9.948 13.107 6.632 24.005 15.636 32.689 27.004 8.687 11.371 15.163 24.638 19.425 39.798zm-46.656 11.243c-.632-8.529-6.519-18.967-11.38-25.457-4.455-5.958-10.846-10.77-17.764-13.502-7.264-2.871-17.133-4.096-25.82-3.713-16.294.714-34.418 6.348-43.57 18.927-9.149 12.575-11.668 33.724-11.334 56.533l109.868-32.788zm226.92 165.94V309.017c0-25.267-4.5-42.955-13.502-53.062-9.002-10.106-21.557-15.16-37.667-15.16-16.421 0-29.053 5.527-37.899 16.58-8.842 11.054-13.264 26.69-13.264 46.906v145.44h-48.796v-248.72h48.796v23.687c6.318-9.16 15.081-16.268 26.293-21.318 11.212-5.054 23.295-7.582 36.244-7.582 29.056 0 51.086 9.319 66.088 27.951 15.001 18.635 22.503 46.43 22.503 83.382v142.6h-48.796zm249.211-65.386c0 13.896-2.607 25.502-7.816 34.821-5.213 9.316-12.08 16.818-20.608 22.504-8.53 5.682-18.4 9.789-29.612 12.317-11.212 2.525-22.977 3.79-35.294 3.79-17.053 0-34.03-3.002-50.93-9.002-16.895-6-31.659-13.582-44.293-22.742l22.265-36.952c11.685 8.214 23.768 14.37 36.244 18.479 12.473 4.104 24.555 6.156 36.24 6.156 30.321 0 45.481-9.157 45.481-27.478 0-9.474-5.13-17.053-15.395-22.738-10.266-5.686-24.4-12.003-42.403-18.953-13.264-5.053-24.555-10.183-33.874-15.395-9.316-5.212-16.898-10.818-22.739-16.818-5.844-6.003-10.027-12.714-12.555-20.134-2.526-7.426-3.79-16.03-3.79-25.82 0-11.053 2.052-20.845 6.159-29.374a57.39 57.39 0 0 1 17.53-21.318c7.578-5.686 16.58-9.949 27.004-12.791 10.421-2.843 21.792-4.266 34.11-4.266 15.792 0 31.031 2.528 45.715 7.582 14.687 5.053 27.557 11.685 38.613 19.896l-21.319 37.428c-10.106-7.264-20.372-12.635-30.793-16.11-10.424-3.471-21.637-5.209-33.64-5.209-12 0-20.686 2.132-26.054 6.394-5.371 4.266-8.055 9.713-8.055 16.345 0 3.16.632 6.08 1.896 8.766 1.264 2.684 3.63 5.448 7.105 8.29 3.475 2.843 8.135 5.765 13.976 8.764 5.844 3.001 13.502 6.397 22.977 10.186 13.899 5.368 26.057 10.5 36.479 15.398 10.424 4.895 19.187 10.422 26.295 16.58 7.106 6.159 12.397 13.344 15.872 21.557 3.472 8.21 5.21 18.159 5.21 29.844v.003zm226.937-57.324c0 20.528-2.842 38.924-8.528 55.19-5.685 16.268-13.581 30.006-23.688 41.218-10.107 11.212-21.95 19.74-35.53 25.581-13.58 5.844-28.426 8.766-44.533 8.766-15.792 0-30.32-3.08-43.585-9.24-13.264-6.158-24.793-14.921-34.585-26.292-9.79-11.37-17.45-24.95-22.977-40.742-5.527-15.792-8.29-33.48-8.29-53.061 0-20.214 2.843-38.376 8.528-54.482 5.683-16.107 13.502-29.768 23.45-40.98 9.948-11.212 21.634-19.74 35.06-25.581 13.422-5.845 28.03-8.767 43.819-8.767 15.792 0 30.479 3.081 44.06 9.24a105.774 105.774 0 0 1 35.294 25.82c9.949 11.053 17.686 24.555 23.213 40.506 5.53 15.948 8.292 33.557 8.292 52.824zm-49.27 1.42c0-25.582-5.605-45.875-16.817-60.88-11.213-15-26.77-22.503-46.666-22.503-18.634 0-33.242 7.502-43.822 22.504-10.58 15.004-15.871 34.824-15.871 59.459 0 27.477 5.685 48.4 17.056 62.771 11.368 14.37 26.372 21.557 45.007 21.557 9.157 0 17.527-2.21 25.108-6.632 7.579-4.421 14.055-10.344 19.423-17.767 5.37-7.423 9.475-16.186 12.317-26.293 2.843-10.107 4.266-20.846 4.266-32.216zm206.557-72.958a82.576 82.576 0 0 0-16.58-6.632c-5.685-1.58-12.635-2.37-20.845-2.37-15.475 0-27.478 5.368-36.006 16.107-8.528 10.742-12.79 26.69-12.79 47.85v142.126h-48.8v-248.72h48.8v23.688c5.682-8.214 13.501-15.081 23.45-20.61 9.947-5.527 21.08-8.29 33.398-8.29 10.106 0 18.4.87 24.873 2.607 6.476 1.734 12.238 4.342 17.291 7.817l-12.79 46.427z" fill="#ef6639" fill-rule="nonzero"/><path d="M1793.665 169.726v86.698h95.226v48.794h-95.23v147.338h-49.74v-331.63h213.662v48.8h-163.918zm172.004 282.83V132.774l48.8-25.585v345.367h-48.8zm297.988-125.543c0 20.528-2.845 38.924-8.53 55.19-5.683 16.268-13.58 30.006-23.686 41.218-10.107 11.212-21.951 19.74-35.533 25.581-13.581 5.844-28.427 8.766-44.533 8.766-15.792 0-30.32-3.08-43.585-9.24-13.264-6.158-24.793-14.921-34.585-26.292-9.79-11.37-17.45-24.95-22.977-40.742-5.527-15.792-8.29-33.48-8.29-53.061 0-20.214 2.842-38.376 8.528-54.482 5.685-16.107 13.502-29.768 23.45-40.98 9.948-11.212 21.636-19.74 35.059-25.581 13.423-5.845 28.03-8.767 43.822-8.767 15.793 0 30.477 3.081 44.058 9.24a105.774 105.774 0 0 1 35.294 25.82c9.948 11.053 17.689 24.555 23.215 40.506 5.527 15.948 8.29 33.557 8.29 52.824h.003zm-49.272 1.42c0-25.582-5.606-45.875-16.819-60.88-11.212-15-26.766-22.503-46.665-22.503-18.635 0-33.242 7.502-43.822 22.504-10.58 15.004-15.872 34.824-15.872 59.459 0 27.477 5.686 48.4 17.057 62.771 11.37 14.37 26.372 21.557 45.007 21.557 9.16 0 17.53-2.21 25.11-6.632 7.58-4.421 14.053-10.344 19.424-17.767 5.368-7.423 9.474-16.186 12.317-26.293s4.263-20.846 4.263-32.216zm289.938 124.123h-41.69l-33.637-125.543a1119.673 1119.673 0 0 1-7.34-29.136c-2.373-9.948-4.187-17.926-5.45-23.926a1156.906 1156.906 0 0 1-5.448 24.161 994.57 994.57 0 0 1-7.344 29.374l-33.163 125.073h-41.69l-68.22-248.724h48.323l29.371 120.808a595.09 595.09 0 0 1 7.105 29.844c2.211 10.425 3.948 18.638 5.213 24.638l6.158-24.635a4044.692 4044.692 0 0 1 7.582-29.847l33.16-120.808h39.324l33.637 121.282c2.54 9.846 4.985 19.717 7.34 29.608 2.37 9.948 4.345 18.083 5.924 24.4 1.578-6.317 3.395-14.607 5.447-24.873a481.266 481.266 0 0 1 6.87-29.609l29.374-120.808h48.323l-69.169 248.721z" fill="#a6a7a9" fill-rule="nonzero"/></svg>)\n",
    "\n",
    "\n",
    "\n",
    "**Lyndon White**\n",
    " - Research Software Engineer -- Invenia Labs, Cambridge\n",
    " - Technically still PhD Candidate -- The University of Western Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  99.2 %0.0 %>]  100.0 %\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/Invenia`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `git@gitlab.invenia.ca:invenia/PackageRegistry.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m WeakRefStrings ‚îÄ v0.5.6\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Manifest.toml`\n",
      " \u001b[90m [a93c6f00]\u001b[39m\u001b[93m ‚Üë DataFrames v0.17.0 ‚áí v0.17.1\u001b[39m\n",
      " \u001b[90m [4c63d2b9]\u001b[39m\u001b[93m ‚Üë StatsFuns v0.7.1 ‚áí v0.8.0\u001b[39m\n",
      " \u001b[90m [bd369af6]\u001b[39m\u001b[93m ‚Üë Tables v0.1.14 ‚áí v0.1.15\u001b[39m\n",
      " \u001b[90m [ea10d353]\u001b[39m\u001b[93m ‚Üë WeakRefStrings v0.5.4 ‚áí v0.5.6\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg: @pkg_str\n",
    "pkg\"activate  .\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "using MLDatasets\n",
    "using Plots\n",
    "using MLDataUtils\n",
    "\n",
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Before TensorFlow\n",
    " \n",
    " - Researchers generally couldn't use Cafe etc as it is not suitably flexible.\n",
    " - One could use Theano, but it is painfully weird\n",
    " - Just write your neural networks by hand, as matrix math\n",
    " - And do your differenciation by hand, first with a blackboard then with more matrix math\n",
    " - While you are at it maybe write your own implementation of gradient descent etc\n",
    " - This was not long ago, I was still doing this in 2014\n",
    " - Julia is a fantasitic language to do this in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Static Graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üëç Easy to manipulate mathematically and easy to think about\n",
    " - It is literally an AST for a language without control flow\n",
    "    - i.e.  a language that is a lot like mathematical notation  \n",
    " - The dervitive of the graph can be calculated  via the chain rule -- generating another graph\n",
    " - Clear seperation of **defintion** from **execution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üëé Dynamic stuctures are impossible.\n",
    "- A dynamic structure is on in which\n",
    " the network structure differs per input\n",
    "- RNNs have to be statically unrolled to their maximum length\n",
    "- If you want to represent say a tree structured network  (e.g. the work of Bowman, Socher and others for NLP)...  üòø"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üîÆ Long-term static graphs are probably going away for good.\n",
    "\n",
    " - Dynamic structure makes deep learning more like normnal programming than math\n",
    " - much more flexible\n",
    " - Even TensorFlow is pushing this way wioth TensorFlow 2.0 and TensorFlow for Swift focusing on **TFEager**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Types of Nodes, i.e. `Tensors`\n",
    " - **Placeholders:** this is where you put your inputs\n",
    " - **Operations:** theres transform inputs into outputs, they do math\n",
    " - **Variables:** thes arre the things you train, they are mutable\n",
    " - **Actions:** These are operations with side effects, like logging (TensorBoard)) and mutating Variable (Optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions\n",
    "\n",
    "Functions mutate **the graph** to introduce nodes.\n",
    "\n",
    "For example:\n",
    " - `sin(::Float64)` in julia would return a `Float64` that is the answer.\n",
    " - `sin(::Tensor)` introduces a `sin` operation into the graph, and returns a `Tensor` that is a reference to it's output, this could be feed to other operations.\n",
    " \n",
    "The answer to that operation is not computed, until you execute the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-06 10:43:35.595159: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Warning: Your Python TensorFlow client version (1.10.0) is below the TensorFlow backend version (1.12.0). This can cause various errors. Please upgrade your Python TensorFlow installation and then restart Julia.\n",
      "‚îÇ You can upgrade by calling `using Conda; Conda.update();` from Julia.\n",
      "‚îî @ TensorFlow /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/version.jl:57\n"
     ]
    }
   ],
   "source": [
    "# Create a summary writer\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "x_summary = TensorFlow.summary.scalar(\"x\", x)\n",
    "y_summary = TensorFlow.summary.scalar(\"y\", y)\n",
    "\n",
    "merged_summary_op = TensorFlow.summary.merge_all()\n",
    "\n",
    "for (ii, x_val) in enumerate(-1:0.1:1)\n",
    "    y_val, summaries = run(sess, [y, merged_summary_op], Dict(x=>x_val))\n",
    "    write(summary_writer, summaries,  ii)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "close(summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "event = TensorFlow.tensorflow.Event()\n",
    "data=convert(Vector{UInt8}, collect(\"\\n,\\n\\x01x\\x12\\vPlaceholder*\\v\\n\\x05dtype\\x12\\x020\\x02*\\r\\n\\x05shape\\x12\\x04:\\x02\\x18\\x01\\n\\x14\\n\\x01y\\x12\\x03Sin\\x1a\\x01x*\\a\\n\\x01T\\x12\\x020\\x02\\x12\\0\\\"\\x02\\b\\x1a\"))\n",
    "setfield!(event, :graph_def, data)\n",
    "write(summary_writer, event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "logdir = \"logs\"\n",
    "mkpath(logdir)\n",
    "\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(logdir; graph=sess.graph)\n",
    "close(summary_writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Node Naming\n",
    "\n",
    " - Notice before I did `@tf begin ... end`\n",
    " - **This is not at all required**\n",
    " - But it does enable automatic node naming\n",
    " - so `@tf y = sin(x)` actually becomes `y = sin(x; name=\"y\")`\n",
    " - This gives you a good graph in tensorboard, and also better error messages.\n",
    " - Further it lets us look up tensors from the graph by **name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.graph[\"x\"] = <Tensor x:1 shape=unknown dtype=Float64>\n",
      "sess.graph[\"y\"] = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show sess.graph[\"x\"]\n",
    "@show sess.graph[\"y\"]\n",
    "\n",
    "run(sess, sess.graph[\"y\"], Dict(sess.graph[\"x\"]=>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets have an exciting Demo\n",
    "\n",
    "![](https://white.ucc.asn.au/posts_assets/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_files/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_28_0.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "RemoteException",
     "evalue": "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
      "",
      "Stacktrace:",
      " [1] #remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421",
      " [2] remotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412",
      " [3] #remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [4] remotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [5] top-level scope at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:189",
      " [6] eval at ./boot.jl:328 [inlined]",
      " [7] eval at ./sysimg.jl:68 [inlined]",
      " [8] add_gradients_py(::Tensor{Float64}, ::Array{Any,1}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1548",
      " [9] gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1536 [inlined] (repeats 2 times)",
      " [10] compute_gradients(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:49",
      " [11] #minimize#1(::Nothing, ::Nothing, ::Nothing, ::Function, ::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:41",
      " [12] minimize(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:38",
      " [13] top-level scope at In[9]:31"
     ]
    }
   ],
   "source": [
    "sess = Session(Graph())\n",
    "\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "\n",
    "# Network Definition\n",
    "@tf begin\n",
    "    X = placeholder(Float32, shape=[-1, 28*28])\n",
    "    \n",
    "    # Network parameters\n",
    "    hl_sizes = [512, 128, 64, 2, 64, 128, 512]\n",
    "\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "    \n",
    "    Wout = get_variable([get_shape(Zs[end], 2), 28*28], Float32)\n",
    "    bout = get_variable([28*28], Float32)\n",
    "    Y = nn.sigmoid(Zs[end]*Wout + bout)\n",
    "    \n",
    "    \n",
    "    Z_code = Zs[end√∑2 + 1] # A name for the coding layer\n",
    "    @assert get_shape(Z_code,2) == 2\n",
    "end\n",
    "\n",
    "losses = 0.5(Y .- X).^2\n",
    "loss = reduce_mean(losses)\n",
    "optimizer = train.minimize(train.AdamOptimizer(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_images = MNIST.traintensor()\n",
    "test_images = MNIST.testtensor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter_image (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function one_image(img::Vector, frames_image_res=30)\n",
    "    ret = zeros((frames_image_res, frames_image_res))\n",
    "    ret[2:end-1, 2:end-1] = 1 .- rotl90(reshape(img, (28,28)))\n",
    "    ret\n",
    "end\n",
    "\n",
    "function scatter_image(images, res; frames_image_res=30, no_overlap=false)\n",
    "    canvas = ones(res, res)\n",
    "    images = reshape(images, (28*28, :));\n",
    "    codes = run(sess, Z_code, Dict(X=>images'))\n",
    "    for ii in 1:2\n",
    "        codes[:,ii] = (codes[:,ii] .- minimum(codes[:,ii]))./(maximum(codes[:,ii])-minimum(codes[:,ii]))\n",
    "        @assert(minimum(codes[:,ii]) >= 0.0)\n",
    "        @assert(maximum(codes[:,ii]) <= 1.0)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    function target_area(code)\n",
    "        central_res = res-frames_image_res-1\n",
    "        border_offset = frames_image_res/2 + 1\n",
    "        x,y = code*central_res .+ border_offset\n",
    "        \n",
    "        get_pos(v) = round(Int, v-frames_image_res/2)\n",
    "        x_min = get_pos(x)\n",
    "        x_max = x_min + frames_image_res-1\n",
    "        y_min =  get_pos(y)\n",
    "        y_max = y_min + frames_image_res-1\n",
    "        \n",
    "        @view canvas[x_min:x_max, y_min:y_max]\n",
    "    end\n",
    "    \n",
    "    for ii in 1:size(codes, 1)\n",
    "        code = codes[ii,:]\n",
    "        img = images[:,ii]\n",
    "        area = target_area(code)        \n",
    "        no_overlap && any(area.<1) && continue # Don't draw over anything\n",
    "        area[:] = one_image(img, frames_image_res)\n",
    "    end\n",
    "    canvas\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: ObsDim not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ObsDim not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at ./In[12]:5"
     ]
    }
   ],
   "source": [
    "#run(sess, global_variables_initializer())\n",
    "auto_loss = Float64[]\n",
    "for epoch in 1:20\n",
    "    epoch_loss = Float64[]\n",
    "    for (batch_ii, batch_x) in  enumerate(eachbatch(train_images, 1_000, ObsDim.Last()))\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "        \n",
    "        if ii % 5 == 1 \n",
    "            println(\"Batch $batch_ii loss: $(loss_o)\")\n",
    "            display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "            IJulia.clear_output(true)\n",
    "        end\n",
    "    end\n",
    "    push!(auto_loss, mean(epoch_loss))\n",
    "    \n",
    "    #\n",
    "#    println(\"Epoch $epoch loss: $(auto_loss[end])\")\n",
    "#    display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "#    IJulia.clear_output(true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets break that example down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining a Custom Activation Function\n",
    "```julia\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "```\n",
    "\n",
    " - Trival in the modern day with Flux, etc\n",
    " - When TensorFlow came out, this was insane wizard tricks, for Cafe users.\n",
    " - But now we take it for granted.\n",
    " - Note that to do this TensorFlow needed to basically implement a full linear algebra and math library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building up our layers\n",
    "\n",
    "```julia\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "```\n",
    "\n",
    "Remember what we are actually doing here is mutating the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLDataUtils for training helpers\n",
    "\n",
    "```julia\n",
    "    for batch_x in eachbatch(train_images, 1_000, ObsDim.Last())\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "    end\n",
    "```\n",
    "\n",
    " - MLDataUtils is a fantastic julia package full of helpers useful with all ML packages\n",
    " - Use it with TensorFlow, use it with Flux, use it with Knet\n",
    " - `eachbatch`/ `batchview`\n",
    " - `eachobs`/`obsview`\n",
    " - Various stratified sampling, `oversample`, `undersample`\n",
    " - test/train splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Complicated Output Layer for HSV Color\n",
    "Saturation and Value are easy, but Hue is angular\n",
    "\n",
    "$$loss =   \\left(y^\\star_{sat} - y_{sat} \\right)^2 + \\left(y^\\star_{val} - y_{val} \\right)^2 \\\\\\ + \\frac{1}{2} \\left(\\sin(y^\\star_{hue}) - y_{shue} \\right)^2 + \\frac{1}{2} \\left(\\cos(y^\\star_{hue}) - y_{chue} \\right)^2 $$\n",
    " \n",
    "<img src=\"./figs/hsv_output_module.png\" width=\"50%\" height=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we build this\n",
    "## (Syntax Overloading)\n",
    "\n",
    "\n",
    "```julia\n",
    "function hsv_output_layer(Y_logit::Tensor{Float32})\n",
    "    # Y_logit size: [missing, 4] \n",
    "\n",
    "    \n",
    "    # Prediction \n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "\n",
    "\n",
    "    # Obs \n",
    "    Y_obs = placeholder(Float32; shape=[-1, 3])\n",
    "    Y_obs_hue = Y_obs[:,1]                       # Notice proper indexing         \n",
    "    Y_obs_sat = Y_obs[:,2]\n",
    "    Y_obs_val = Y_obs[:,3]\n",
    "\n",
    "    Y_obs_shue = sin(Float32(2œÄ) .* Y_obs_hue)\n",
    "    Y_obs_chue = cos(Float32(2œÄ) .* Y_obs_hue)\n",
    "    \n",
    "    \n",
    "    # Loss                        \n",
    "    loss_hue = 0.5reduce_mean((Y_shue - Y_obs_shue)^2 + (Y_chue - Y_obs_chue)^2))\n",
    "    loss_sat = reduce_mean((Y_sat-Y_obs_sat)^2)\n",
    "    loss_val = reduce_mean((Y_val-Y_obs_val)^2)\n",
    "\n",
    "    loss_total = identity(loss_hue + loss_sat + loss_val)\n",
    "\n",
    "                        \n",
    "    # For Output, we want hue angle measured in 0:1 (units of turns)\n",
    "    Y_hue_o1 = Ops.atan2(Y_shue, Y_chue)/(2Float32(œÄ))\n",
    "    Y_hue_o2 = select(Y_hue_o1 > 0, Y_hue_o1, Y_hue_o1+1) # Wrap around things below 0\n",
    "    Y_hue = reshape(Y_hue_o2, [-1]) # force shape\n",
    "\n",
    "    Y = identity([Y_hue Y_sat Y_val]) # *** Notice Julia Style hcat***\n",
    "\n",
    "    return loss_total\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overloading hcat & vcat\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    " Y = identity([Y_hue Y_sat Y_val])\n",
    "```\n",
    "\n",
    "\n",
    "So that `[a b]` and `[a; b]` work.\n",
    "vs Base Tensorflow, would have you first make sure everything is the same number of dimensions,\n",
    "then `concat` them,\n",
    "And you couldn't use julia style syntax.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/7099f05f523556829164aab41eccd394d29df898/src/ops/transformations.jl#L129-L150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overloading getindex\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "```\n",
    "\n",
    "Indexing with slices and ranges is much nicer than `tf.gather` and `tf.gather_nd` and even than `tf.slice`.\n",
    "\n",
    "So that `X[a:b]`, `X[a]`, `X[:, end√∑2]` etc.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/master/src/ops/indexing.jl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow.jl Conventions vs Julia Conventions vs Python TensorFlow Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Julia**: 1-based indexing   \n",
    "**Python TF**: 0-based indexing  \n",
    "**TensorFlow.jl**: 1-based indexing   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:** explicit broadcasting   \n",
    "**Python TF:** implicit broadcasting   \n",
    "**TensorFlow.jl:** implicit or explicit broadcasting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:**  last index at `end`, 2nd last in `end-1`, etc.   \n",
    "**Python TF:** last index at `-1` second last in `-2`   \n",
    "**TensorFlow.jl** last index at `end` 2nd last in `end-1`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:**  Operations in Julia ecosystem namespaces. (`SVD` in `LinearAlgebra`, `erfc` in `SpecialFunctions`, `cos` in `Base`)   \n",
    "**Python TF:** All operations in TensorFlow's namespaces (`SVD` in `tf.linalg`, `erfc` in `tf.math`, `cos` in `tf.math`, and all reexported from `tf`)  \n",
    "**TensorFlow.jl**  Existing Julia functions overloaded to call TensorFlow equivalents when called with TensorFlow arguments  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:** Container types are parametrized by number of dimensions and element type   \n",
    "**Python TF:** N/A -- python does not have a parametric type system   \n",
    "**TensorFlow.jl:** Tensors are parametrized by element type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are the bits that make TensorFlow.jl work defined?\n",
    "\n",
    "## TensorFlow.jl (Julia)\n",
    " - Nice Things\n",
    " - RNNs\n",
    " - Training / Optimizers\n",
    " \n",
    "## TensorFlow (PyCall)\n",
    " - Gradients\n",
    " - Writing tensorboard events to file\n",
    " \n",
    "## LibTensorFlow (C API)\n",
    " - Operations\n",
    " - Shape Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# üò¢ BatchNorm\n",
    "\n",
    " - There is a `BatchNorm` op in LibTensorFlow\n",
    " - Actually there are several, for different parts of the Fusing.\n",
    " - to get `BatchNorm` to work, you need to glue these together with the right predeclared variable for state and for reused working memory\n",
    " - This is hundreds (thousands?) of lines of python glue code, that needs to be reimplemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  üò¢ Windows Support\n",
    "\n",
    " - I've not tried to get this working in  a while but last time:\n",
    " - Unending segfaults on basic operations.\n",
    " - In theory it should just work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  TFEager\n",
    "## Work In Progress\n",
    "\n",
    " - Jon Malmaud is working on this\n",
    " - Google apparently wants this.\n",
    " - But why? I have a perfectly nice eager NN framework called Flux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dropping the Python Dependency\n",
    " - Python dependency is a nasty hack\n",
    " - It is basically only used for getting gradients.\n",
    " - we actually interact with it primarily by:\n",
    "     - exporting the graph\n",
    "     - running some Python TF on it\n",
    "     - Importing the modified graph back\n",
    "     \n",
    " - We need it for gradients as they are not in the C API\n",
    " - They are coming to the C API, but not ready yet.]]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Invenia Labs\n",
    "\n",
    "![](https://www.invenia.ca/wp-content/themes/relish_theme/img/labs-logo.png)\n",
    "\n",
    "## We're hiring\n",
    "### People who know Julia\n",
    "### People who know Machine Learning\n",
    "I have left some fliers about open positions at the entrance."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
