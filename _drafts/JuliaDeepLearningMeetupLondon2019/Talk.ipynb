{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow and other tools for ML in Julia\n",
    "\n",
    "**Lyndon White**\n",
    " - Research Software Engineer -- Invenia Labs, Cambridge\n",
    " - Technically still PhD Candidate -- The University of Western Australia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  99.2 %0.0 %>]  100.0 %\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/Invenia`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `git@gitlab.invenia.ca:invenia/PackageRegistry.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m WeakRefStrings ─ v0.5.6\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/Manifest.toml`\n",
      " \u001b[90m [a93c6f00]\u001b[39m\u001b[93m ↑ DataFrames v0.17.0 ⇒ v0.17.1\u001b[39m\n",
      " \u001b[90m [4c63d2b9]\u001b[39m\u001b[93m ↑ StatsFuns v0.7.1 ⇒ v0.8.0\u001b[39m\n",
      " \u001b[90m [bd369af6]\u001b[39m\u001b[93m ↑ Tables v0.1.14 ⇒ v0.1.15\u001b[39m\n",
      " \u001b[90m [ea10d353]\u001b[39m\u001b[93m ↑ WeakRefStrings v0.5.4 ⇒ v0.5.6\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg: @pkg_str\n",
    "pkg\"activate  .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/MLDataUtils/CQWB9.ji for MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "│ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "┌ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "│ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "┌ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Warning: Module Compat with build ID 176874926020307 is missing from the cache.\n",
      "│ This may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:947\n",
      "┌ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/DataStreams/E9VAJ.ji for DataStreams [9a8bc11e-79be-5b39-94d7-1ccc349a1a85]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /Users/oxinabox/.julia/compiled/v1.1/Tables/Z804B.ji for Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "ename": "CompositeException",
     "evalue": "On worker 2:\nArgumentError: Package MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\n_require at ./loading.jl:929\nrequire at ./loading.jl:858\n#2 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/Distributed.jl:77\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259\n#remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421\nremotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412\n#remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\nremotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\n(::getfield(Distributed, Symbol(\"##1#3\")){Base.PkgId})() at ./task.jl:259",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nArgumentError: Package MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\n_require at ./loading.jl:929\nrequire at ./loading.jl:858\n#2 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/Distributed.jl:77\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259\n#remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421\nremotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412\n#remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\nremotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433\n(::getfield(Distributed, Symbol(\"##1#3\")){Base.PkgId})() at ./task.jl:259",
      "",
      "Stacktrace:",
      " [1] sync_end(::Array{Any,1}) at ./task.jl:226",
      " [2] _require_callback(::Base.PkgId) at ./task.jl:245",
      " [3] #invokelatest#1 at ./essentials.jl:742 [inlined]",
      " [4] invokelatest at ./essentials.jl:741 [inlined]",
      " [5] require(::Base.PkgId) at ./loading.jl:861",
      " [6] require(::Module, ::Symbol) at ./loading.jl:853",
      " [7] top-level scope at In[8]:3"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Plots\n",
    "using MLDataUtils\n",
    "\n",
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before TensorFlow\n",
    " \n",
    " - Researchers generally couldn't use Cafe etc as it is not suitably flexible.\n",
    " - One could use Theano, but it is painfully weird\n",
    " - Just write your neural networks by hand, as matrix math\n",
    " - And do your differenciation by hand, first with a blackboard then with more matrix math\n",
    " - While you are at it maybe write your own implementation of gradient descent etc\n",
    " - This was not long ago, I was still doing this in 2014\n",
    " - Julia is a fantasitic language to do this in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Static Graphs\n",
    "\n",
    " - Easy to manipulate mathematically and easy to think about\n",
    "     - It is literally an AST for a language without control flow\n",
    "        - i.e.  a language that is a lot like mathematical notation  \n",
    "     - The dervitive of the graph can be calculated  via the chain rule -- generating another graph\n",
    " - Dynamic stuctures are impossible\n",
    "     - A dynamic structure is on in which the network structure differs per input\n",
    "     - RNNs have to be statically unrolled to their maximum length\n",
    "     - If you want to represent say a tree structured network  (e.g. the work of Bowman, Socher and others for NLP)...  **Cry**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Types of Nodes, i.e. `Tensors`\n",
    " - **Placeholders:** this is where you put your inputs\n",
    " - **Operations:** theres transform inputs into outputs, they do math\n",
    " - **Variables:** thes arre the things you train, they are mutable\n",
    " - **Actions:** These are operations with side effects, like logging (TensorBoard)) and mutating Variable (Optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions\n",
    "\n",
    "Functions mutate **the graph** to introduce nodes.\n",
    "\n",
    "For example:\n",
    " - `sin(::Float64)` in julia would return a `Float64` that is the answer.\n",
    " - `sin(::Tensor)` introduces a `sin` operation into the graph, and returns a `Tensor` that is a reference to it's output, this could be feed to other operations.\n",
    " \n",
    "The answer to that operation is not computed, until you execute the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-06 10:43:35.595159: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Your Python TensorFlow client version (1.10.0) is below the TensorFlow backend version (1.12.0). This can cause various errors. Please upgrade your Python TensorFlow installation and then restart Julia.\n",
      "│ You can upgrade by calling `using Conda; Conda.update();` from Julia.\n",
      "└ @ TensorFlow /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/version.jl:57\n"
     ]
    }
   ],
   "source": [
    "# Create a summary writer\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "@show y\n",
    "\n",
    "run(sess, y, Dict(x=>0.5))\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "x_summary = TensorFlow.summary.scalar(\"x\", x)\n",
    "y_summary = TensorFlow.summary.scalar(\"y\", y)\n",
    "\n",
    "merged_summary_op = TensorFlow.summary.merge_all()\n",
    "\n",
    "for (ii, x_val) in enumerate(-1:0.1:1)\n",
    "    y_val, summaries = run(sess, [y, merged_summary_op], Dict(x=>x_val))\n",
    "    write(summary_writer, summaries,  ii)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(mkpath(\"logs\"); graph=sess.graph)\n",
    "close(summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = TensorFlow.tensorflow.Event()\n",
    "data=convert(Vector{UInt8}, collect(\"\\n,\\n\\x01x\\x12\\vPlaceholder*\\v\\n\\x05dtype\\x12\\x020\\x02*\\r\\n\\x05shape\\x12\\x04:\\x02\\x18\\x01\\n\\x14\\n\\x01y\\x12\\x03Sin\\x1a\\x01x*\\a\\n\\x01T\\x12\\x020\\x02\\x12\\0\\\"\\x02\\b\\x1a\"))\n",
    "setfield!(event, :graph_def, data)\n",
    "write(summary_writer, event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using TensorFlow: summary\n",
    "logdir = \"logs\"\n",
    "mkpath(logdir)\n",
    "\n",
    "sess= Session(Graph())\n",
    "\n",
    "@tf begin\n",
    "    x = placeholder(Float64)\n",
    "    y = sin(x)\n",
    "end\n",
    "\n",
    "summary_writer = TensorFlow.summary.FileWriter(logdir; graph=sess.graph)\n",
    "close(summary_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Node Naming\n",
    "\n",
    " - Notice before I did `@tf begin ... end`\n",
    " - **This is not at all required**\n",
    " - But it does enable automatic node naming\n",
    " - so `@tf y = sin(x)` actually becomes `y = sin(x; name=\"y\")`\n",
    " - This gives you a good graph in tensorboard, and also better error messages.\n",
    " - Further it lets us look up tensors from the graph by **name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.graph[\"x\"] = <Tensor x:1 shape=unknown dtype=Float64>\n",
      "sess.graph[\"y\"] = <Tensor y:1 shape=unknown dtype=Float64>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.479425538604203"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show sess.graph[\"x\"]\n",
    "@show sess.graph[\"y\"]\n",
    "\n",
    "run(sess, sess.graph[\"y\"], Dict(sess.graph[\"x\"]=>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets have an exciting Demo\n",
    "\n",
    "![](https://white.ucc.asn.au/posts_assets/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_files/Intro%20to%20Machine%20Learning%20with%20TensorFlow.jl_28_0.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "RemoteException",
     "evalue": "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
     "output_type": "error",
     "traceback": [
      "On worker 2:\nPython error: PyObject ValueError(\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Mul/Cast = Cast[DstT=DT_DOUBLE, SrcT=DT_FLOAT, Truncate=false](Add). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\",)\nerror at ./error.jl:33\n#3 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:45\npy_with at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:20\nmake_py_graph at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:52\npy_gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/py.jl:75\n#65 at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:190\n#116 at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:276\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:56\nrun_work_thunk at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/process_messages.jl:65\n#102 at ./task.jl:259",
      "",
      "Stacktrace:",
      " [1] #remotecall_wait#154(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:421",
      " [2] remotecall_wait(::Function, ::Distributed.Worker) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:412",
      " [3] #remotecall_wait#157(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [4] remotecall_wait(::Function, ::Int64) at /Users/osx/buildbot/slave/package_osx64/build/usr/share/julia/stdlib/v1.1/Distributed/src/remotecall.jl:433",
      " [5] top-level scope at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/TensorFlow.jl:189",
      " [6] eval at ./boot.jl:328 [inlined]",
      " [7] eval at ./sysimg.jl:68 [inlined]",
      " [8] add_gradients_py(::Tensor{Float64}, ::Array{Any,1}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1548",
      " [9] gradients at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/core.jl:1536 [inlined] (repeats 2 times)",
      " [10] compute_gradients(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}, ::Nothing) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:49",
      " [11] #minimize#1(::Nothing, ::Nothing, ::Nothing, ::Function, ::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:41",
      " [12] minimize(::TensorFlow.train.AdamOptimizer, ::Tensor{Float64}) at /Users/oxinabox/Documents/oxinabox.github.io/_drafts/JuliaDeepLearningMeetupLondon2019/dev/TensorFlow/src/train.jl:38",
      " [13] top-level scope at In[9]:31"
     ]
    }
   ],
   "source": [
    "sess = Session(Graph())\n",
    "\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "\n",
    "# Network Definition\n",
    "@tf begin\n",
    "    X = placeholder(Float32, shape=[-1, 28*28])\n",
    "    \n",
    "    # Network parameters\n",
    "    hl_sizes = [512, 128, 64, 2, 64, 128, 512]\n",
    "\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "    \n",
    "    Wout = get_variable([get_shape(Zs[end], 2), 28*28], Float32)\n",
    "    bout = get_variable([28*28], Float32)\n",
    "    Y = nn.sigmoid(Zs[end]*Wout + bout)\n",
    "    \n",
    "    \n",
    "    Z_code = Zs[end÷2 + 1] # A name for the coding layer\n",
    "    @assert get_shape(Z_code,2) == 2\n",
    "end\n",
    "\n",
    "losses = 0.5(Y .- X).^2\n",
    "loss = reduce_mean(losses)\n",
    "optimizer = train.minimize(train.AdamOptimizer(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = MNIST.traintensor()\n",
    "test_images = MNIST.testtensor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter_image (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function one_image(img::Vector, frames_image_res=30)\n",
    "    ret = zeros((frames_image_res, frames_image_res))\n",
    "    ret[2:end-1, 2:end-1] = 1 .- rotl90(reshape(img, (28,28)))\n",
    "    ret\n",
    "end\n",
    "\n",
    "function scatter_image(images, res; frames_image_res=30, no_overlap=false)\n",
    "    canvas = ones(res, res)\n",
    "    images = reshape(images, (28*28, :));\n",
    "    codes = run(sess, Z_code, Dict(X=>images'))\n",
    "    for ii in 1:2\n",
    "        codes[:,ii] = (codes[:,ii] .- minimum(codes[:,ii]))./(maximum(codes[:,ii])-minimum(codes[:,ii]))\n",
    "        @assert(minimum(codes[:,ii]) >= 0.0)\n",
    "        @assert(maximum(codes[:,ii]) <= 1.0)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    function target_area(code)\n",
    "        central_res = res-frames_image_res-1\n",
    "        border_offset = frames_image_res/2 + 1\n",
    "        x,y = code*central_res .+ border_offset\n",
    "        \n",
    "        get_pos(v) = round(Int, v-frames_image_res/2)\n",
    "        x_min = get_pos(x)\n",
    "        x_max = x_min + frames_image_res-1\n",
    "        y_min =  get_pos(y)\n",
    "        y_max = y_min + frames_image_res-1\n",
    "        \n",
    "        @view canvas[x_min:x_max, y_min:y_max]\n",
    "    end\n",
    "    \n",
    "    for ii in 1:size(codes, 1)\n",
    "        code = codes[ii,:]\n",
    "        img = images[:,ii]\n",
    "        area = target_area(code)        \n",
    "        no_overlap && any(area.<1) && continue # Don't draw over anything\n",
    "        area[:] = one_image(img, frames_image_res)\n",
    "    end\n",
    "    canvas\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: ObsDim not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ObsDim not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at ./In[12]:5"
     ]
    }
   ],
   "source": [
    "#run(sess, global_variables_initializer())\n",
    "auto_loss = Float64[]\n",
    "for epoch in 1:20\n",
    "    epoch_loss = Float64[]\n",
    "    for (batch_ii, batch_x) in  enumerate(eachbatch(train_images, 1_000, ObsDim.Last()))\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "        \n",
    "        if ii % 5 == 1 \n",
    "            println(\"Batch $batch_ii loss: $(loss_o)\")\n",
    "            display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "            IJulia.clear_output(true)\n",
    "        end\n",
    "    end\n",
    "    push!(auto_loss, mean(epoch_loss))\n",
    "    \n",
    "    #\n",
    "#    println(\"Epoch $epoch loss: $(auto_loss[end])\")\n",
    "#    display(heatmap(scatter_image(test_images[:,:,1:100], 700)))\n",
    "#    IJulia.clear_output(true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets break that example down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining a Custom Activation Function\n",
    "```\n",
    "leaky_relu6(x) = 0.01x + nn.relu6(x)\n",
    "```\n",
    "\n",
    " - Trival in the modern day with Flux, etc\n",
    " - When TensorFlow came out, this was insane wizard tricks, for Cafe users.\n",
    " - But now we take it for granted.\n",
    " - Note that to do this TensorFlow needed to basically implement a full linear algebra and math library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building up our layers\n",
    "\n",
    "```\n",
    "    Zs = [X]\n",
    "    for (ii, hlsize) in enumerate(hl_sizes)\n",
    "        Wii = get_variable(\"W_$ii\", [get_shape(Zs[end], 2), hlsize], Float32)\n",
    "        bii = get_variable(\"b_$ii\", [hlsize], Float32)\n",
    "        Zii = leaky_relu6(Zs[end]*Wii + bii)\n",
    "        push!(Zs, Zii)\n",
    "    end\n",
    "```\n",
    "\n",
    "Remember what we are actually doing here is mutating the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLDataUtils for training helpers\n",
    "\n",
    "```\n",
    "    for batch_x in eachbatch(train_images, 1_000, ObsDim.Last())\n",
    "        flat_batch_x = reshape(batch_x, (28*28, :))\n",
    "        loss_o, _ = run(sess, (loss, optimizer), Dict(X=>flat_batch_x'))\n",
    "        push!(epoch_loss, loss_o)\n",
    "    end\n",
    "```\n",
    "\n",
    " - MLDataUtils is a fantastic julia package full of helpers useful with all ML packages\n",
    " - Use it with TensorFlow, use it with Flux, use it with Knet\n",
    " - `eachbatch`/ `batchview`\n",
    " - `eachobs`/`obsview`\n",
    " - Various stratified sampling, `oversample`, `undersample`\n",
    " - test/train splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Complicated Output Layer for HSV Color\n",
    " - Saturation and Value are easy, but Hue is angular\n",
    "$$\n",
    "loss =\n",
    "\\frac{1}{2} \\left(\\sin(y^\\star_{hue}) - y_{shue} \\right)^2     \n",
    "+ \\frac{1}{2} \\left(\\cos(y^\\star_{hue}) - y_{chue} \\right)^2  \n",
    "+ \\left(y^\\star_{sat} - y_{sat} \\right)^2  \n",
    "+ \\left(y^\\star_{val} - y_{val} \\right)^2 %\n",
    "$$\n",
    " \n",
    "<img src=\"./figs/hsv_output_module.png\" width=\"50%\" height=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How do we build this\n",
    "## (Syntax Overloading)\n",
    "\n",
    "\n",
    "```julia\n",
    "function hsv_output_layer(Y_logit::Tensor{Float32})\n",
    "    # Y_logit size: [missing, 4] \n",
    "\n",
    "    \n",
    "    # Prediction \n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "\n",
    "\n",
    "    # Obs \n",
    "    Y_obs = placeholder(Float32; shape=[-1, 3])\n",
    "    Y_obs_hue = Y_obs[:,1]                       # Notice proper indexing         \n",
    "    Y_obs_sat = Y_obs[:,2]\n",
    "    Y_obs_val = Y_obs[:,3]\n",
    "\n",
    "    Y_obs_shue = sin(Float32(2π) .* Y_obs_hue)\n",
    "    Y_obs_chue = cos(Float32(2π) .* Y_obs_hue)\n",
    "    \n",
    "    \n",
    "    # Loss                        \n",
    "    loss_hue = 0.5reduce_mean((Y_shue - Y_obs_shue)^2 + (Y_chue - Y_obs_chue)^2))\n",
    "    loss_sat = reduce_mean((Y_sat-Y_obs_sat)^2)\n",
    "    loss_val = reduce_mean((Y_val-Y_obs_val)^2)\n",
    "\n",
    "    loss_total = identity(loss_hue + loss_sat + loss_val)\n",
    "\n",
    "                        \n",
    "    # For Output, we want hue angle measured in 0:1 (units of turns)\n",
    "    Y_hue_o1 = Ops.atan2(Y_shue, Y_chue)/(2Float32(π))\n",
    "    Y_hue_o2 = select(Y_hue_o1 > 0, Y_hue_o1, Y_hue_o1+1) # Wrap around things below 0\n",
    "    Y_hue = reshape(Y_hue_o2, [-1]) # force shape\n",
    "\n",
    "    Y = identity([Y_hue Y_sat Y_val]) # *** Notice Julia Style hcat***\n",
    "\n",
    "    return loss_total\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overloading hcat & vcat\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    " Y = identity([Y_hue Y_sat Y_val])\n",
    "```\n",
    "\n",
    "\n",
    "So that `[a b]` and `[a; b]` work.\n",
    "vs Base Tensorflow, would have you first make sure everything is the same number of dimensions,\n",
    "then `concat` them,\n",
    "And you couldn't use julia style syntax.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/7099f05f523556829164aab41eccd394d29df898/src/ops/transformations.jl#L129-L150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overloading getindex\n",
    "\n",
    "Like in:\n",
    "\n",
    "```julia\n",
    "    Y_sat = nn.sigmoid(Y_logit[:,3])  # range 0:1\n",
    "    Y_val = nn.sigmoid(Y_logit[:,4])  # range 0:1\n",
    "\n",
    "    Y_shue = tanh(Y_logit[:,1])       # range -1:1 -- like sin\n",
    "    Y_chue = tanh(Y_logit[:,2])       # range -1:1 -- like cos\n",
    "```\n",
    "\n",
    "Indexing with slices and ranges is much nicer than `tf.gather` and `tf.gather_nd` and even than `tf.slice`.\n",
    "\n",
    "So that `X[a:b]`, `X[a]`, `X[:, end÷2]` etc.\n",
    "\n",
    "https://github.com/malmaud/TensorFlow.jl/blob/master/src/ops/indexing.jl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow.jl Conventions vs Julia Conventions vs Python TensorFlow Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Julia**: 1-based indexing   \n",
    "**Python TF**: 0-based indexing  \n",
    "**TensorFlow.jl**: 1-based indexing   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:** explicit broadcasting   \n",
    "**Python TF:** implicit broadcasting   \n",
    "**TensorFlow.jl:** implicit or explicit broadcasting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Julia:**  last index at `end`, 2nd last in `end-1`, etc.   \n",
    "**Python TF:** last index at `-1` second last in `-2`   \n",
    "**TensorFlow.jl** last index at `end` 2nd last in `end-1`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:**  Operations in Julia ecosystem namespaces. (`SVD` in `LinearAlgebra`, `erfc` in `SpecialFunctions`, `cos` in `Base`)   \n",
    "**Python TF:** All operations in TensorFlow's namespaces (`SVD` in `tf.linalg`, `erfc` in `tf.math`, `cos` in `tf.math`, and all reexported from `tf`)  \n",
    "**TensorFlow.jl**  Existing Julia functions overloaded to call TensorFlow equivalents when called with TensorFlow arguments  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Julia:** Container types are parametrized by number of dimensions and element type   \n",
    "**Python TF:** N/A -- python does not have a parametric type system   \n",
    "**TensorFlow.jl:** Tensors are parametrized by element type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are the bits that make TensorFlow.jl work defined?\n",
    "\n",
    "## TensorFlow.jl (Julia)\n",
    " - Nice Things\n",
    " - RNNs\n",
    " - Training / Optimizers\n",
    " \n",
    "## TensorFlow (PyCall)\n",
    " - Gradients\n",
    " - Writing tensorboard events to file\n",
    " \n",
    "## LibTensorFlow (C API)\n",
    " - Operations\n",
    " - Shape Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 😢 BatchNorm\n",
    "\n",
    " - There is a `BatchNorm` op in LibTensorFlow\n",
    " - Actually there are several, for different parts of the Fusing.\n",
    " - to get `BatchNorm` to work, you need to glue these together with the right predeclared variable for state and for reused working memory\n",
    " - This is hundreds (thousands?) of lines of python glue code, that needs to be reimplemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  😢 Windows Support\n",
    "\n",
    " - I've not tried to get this working in  a while but last time:\n",
    " - Unending segfaults on basic operations.\n",
    " - In theory it should just work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  TFEager\n",
    "## Work In Progress\n",
    "\n",
    " - Jon Malmaud is working on this\n",
    " - Google apparently wants this.\n",
    " - But why? I have a perfectly nice eager NN framework called Flux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropping the Python Dependency\n",
    " - Python dependency is a nasty hack\n",
    " - It is basically only used for getting gradients.\n",
    " - we actually interact with it primarily by:\n",
    "     - exporting the graph\n",
    "     - running some Python TF on it\n",
    "     - Importing the modified graph back\n",
    "     \n",
    " - We need it for gradients as they are not in the C API\n",
    " - They are coming to the C API, but not ready yet.]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Invenia Labs\n",
    "\n",
    "![](https://www.invenia.ca/wp-content/themes/relish_theme/img/labs-logo.png)\n",
    "\n",
    "## We're hiring\n",
    "### People who know Julia\n",
    "### People who know Machine Learning\n",
    "I have left some fliers about open positions at the entrance."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
