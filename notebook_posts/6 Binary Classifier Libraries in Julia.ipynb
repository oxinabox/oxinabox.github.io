{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wished to do some machine learning for binary classification.\n",
    "Binary classification is perhaps the most basic of all supervised learning problems.\n",
    "Unsurprisingly julia has many libraries for it.\n",
    "Today we are looking at:\n",
    "LIBLINEAR (linear SVMs), LIBSVM (Kernel SVM), XGBoost (Extreme Gradient Boosting), DecisionTrees (RandomForests), Flux (neural networks), TensorFlow (also neural networks).\n",
    "Most (infact I think all) of these do other things as well.\n",
    "In this post we are only concentrating on their ability to be used for binary classification.\n",
    "We'll also not really be going into exploring all their options (e.g. different types of kernals).\n",
    "Furthermore, I'm not rigeriously tuning the hyperparameters so this can't be considered a fair test for performance.\n",
    "I'm also not performing preprocessing (e.g. many classifies like it if you standarise your features to zero mean unit variance).\n",
    "You can look at this post more as talking above *what code for that package looks like*, and this is roughly how long it takes and how well it does out of the box.\n",
    "<!--more-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task: Predict if that part of the Australian Flag is Blue\n",
    "\n",
    "This is on the mildly gnarly side of binary classification problems.\n",
    "The classifying regions are:\n",
    "\n",
    " - Not linearly seperable\n",
    "    - you can't draw a line such that on one since is all the blue parts and on the other is all the nonblue parts.\n",
    " - Not connected\n",
    "    - the stars for example are not entirely separated by blue background regions\n",
    " - Not convex \n",
    "    - with in a section of one color, you can draw a line between two points and have it exit that section, then reenter.\n",
    " - Unbalanced classes\n",
    "    - most of the image is blue)\n",
    " \n",
    "So it seams like a good problem.\n",
    "\n",
    "![Australian Flag](https://upload.wikimedia.org/wikipedia/en/thumb/b/b9/Flag_of_Australia.svg/800px-Flag_of_Australia.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "An image of the flag gives us one datum per pixel.\n",
    "We're going to sample that, just so that plotting is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Images, FileIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0"
     ]
    }
   ],
   "source": [
    "img = load(download(\"https://upload.wikimedia.org/wikipedia/en/thumb/b/b9/Flag_of_Australia.svg/320px-Flag_of_Australia.svg.png\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isblue (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isblue(pixel) = pixel.b > pixel.r && pixel.b > pixel.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33m!(B::BitArray) is deprecated, use .!(B) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1m!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::BitArray{2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/wheel/oxinabox/.julia/v0.6/Compat/src/Compat.jl:174\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/wheel/oxinabox/.julia/v0.6/IJulia/src/execute_request.jl:154\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/wheel/oxinabox/.julia/v0.6/IJulia/src/eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##14#17)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[4], in expression starting on line 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAACgAQAAAABV2IgxAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAAHdihOkAAAAB3RJTUUH4QwOCCIDBEu+lwAAAoFJREFUWMPtmEGK1UAQhtMGJi7EzA36CoKbARfRmzg38ASToOC5GteCFxDMynXAzRPCK6c7qaS7q6othjc4YP5VeP0l/ac7qfrzKgBXeVmAUzgwABUngCkFGxGcU9CKIKRgL4MuBg3I4BSDTQGcY9AWQIjBXgL9wLCD3uKZBe1qcgUbtEKEIyuI51GZ1eQK9rhcvEmHIJ5WMLmAssVtbAFli9tsC1iwiIMBLFnE6QJYsogmA1iyiCYDmFs0AzXpQWIxA8OEHiQWbz9Skx4kFt98pyY9SFaxy5bAAz8AfpNVtCe6kqvmItjs4CSCwGj3dNaBZnmdFFdsRuXU7aQEX35Qgi9eKcHn10ow1gEe4JMArx8L9BJLStDNfmh3kJb6qxu3HfcglD2vei+ZciFdwCG2yJfm5TKJRb7YZ2CY0XLtI5260JC8Whhji3KL6wDr8zJqs6a5N62tPC/z2awNU1Bo7O+27vYzsUiiwuuvCH5JLJLwsXe3T4lFEme2XmRMYpEEpLy7iZGLgDhisxCXd2AxFpJNR/c2C5rkiiBE17gNIjjFIIbhuA0i6GIQ43XcBhGsYhADe9wGV5D/BIjb4ApOKSh/VLgUNIVg+DSVBcMLgGQnJZFnQ1LtLg1evVeChx5Pg5K7/COqBmv3z0Dt2+GasVaBp2a0KvCune5UYPft16wDS9/BsbZC+je1pQ/mB4H3NX5UgXXhX4KHgeqp1TejXnAxf2cKYWXQ3YvubkJwVm0265F9hbl7uTx4+1kJdtwenHtmLbh1mDsdaE520IFTS0HOoxkbCrJL5moCGu454UC2wpmB2QYuURiuf5BseK9nVfWW/EiyoSSSDQ8d+u/1B+SA2EtVd8WVAAAAAElFTkSuQmCC",
      "text/plain": [
       "160×320 ColorView{Gray}(::BitArray{2}) with element type ColorTypes.Gray{Bool}:\n",
       " Gray{Bool}(true)   Gray{Bool}(true)   …  Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)   …  Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(true)   Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(true)      Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)  …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " ⋮                                     ⋱                   \n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)  …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)  …  Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)\n",
       " Gray{Bool}(false)  Gray{Bool}(false)     Gray{Bool}(false)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorview(Gray, !(isblue.(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×51200 Array{Any,2}:\n",
       "     1.0      2.0      3.0      4.0  …   157.0   158.0   159.0   160.0\n",
       "     1.0      1.0      1.0      1.0      320.0   320.0   320.0   320.0\n",
       " false    false    false    false       true    true    true    true  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const all_feature1 = Vector{Float64}()\n",
    "const all_feature2 = Vector{Float64}()\n",
    "const all_labels = Vector{Bool}()\n",
    "\n",
    "@inbounds for ind in eachindex(IndexCartesian(), img)\n",
    "    pixel = img[ind]\n",
    "    push!(all_labels, isblue(pixel))\n",
    "    push!(all_feature1, ind.I[1])\n",
    "    push!(all_feature2, ind.I[2])\n",
    "end\n",
    "\n",
    "const all_features = [all_feature1'; all_feature2']\n",
    "# standard julia Observations are in final index form (i.e columns of matrixes)\n",
    "Any[all_features; all_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally I would do this data munging using MLDataUtils.jl, which I have [blogged about before](http://white.ucc.asn.au/2017/01/24/JuliaML-and-TensorFlow-Tuitorial.html) (though it might be nice to few more posts about it, it is a great package, and I don't know that I've fully covered its capacities).\n",
    "\n",
    "But since I am already about to introduce 6 packages, I thought I would minimize talking about other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const all_inds = shuffle(1:length(all_labels))\n",
    "const test_inds = all_inds[1:end÷5] # first 20%\n",
    "const train_inds = all_inds[9end÷10:end] # last 10%\n",
    "\n",
    "const test_features = all_features[:, test_inds]\n",
    "const test_labels = all_labels[test_inds]\n",
    "\n",
    "const train_features = all_features[:, train_inds]\n",
    "const train_labels = all_labels[train_inds];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "gr()\n",
    "\n",
    "function plotflag(xs,ys)\n",
    "    scatter(xs[2,:],-xs[1,:]; zcolor=ys,\n",
    "    markersize=2, markerstrokealpha=0, bg=colorant\"gray\", seriescolor=:blues)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotflag(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotflag(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface\n",
    "As was discussed on the julia slack yesterday.\n",
    "There is a real problem with a lack of consistency.\n",
    "\n",
    "So I am going to take a leaf from XKCD #927,\n",
    "and define one.\n",
    "![Standards](https://imgs.xkcd.com/comics/standards.png)\n",
    "\n",
    " - `StatsBase.fit!(modeltype, features, labels)` returns a model of that type that is trained on those features and labels.\n",
    "    - Since we are only interested in binary classification, labels witll be an `AbstractVector{Bool}` with one entry per column of the feature matrix\n",
    " - `StatsBase.predict(model, features)` returns a vector of estimated probabilities of classification being true\n",
    "    - one entry per column in features.\n",
    "\n",
    "Something like this is actually in use in a bunch of places already, just not these packages, it seems..\n",
    "Some packages (`LibSVM`, `DecisionTrees.jl`) use the same names, from [`ScikitLearnBase`](https://github.com/cstjean/ScikitLearnBase.jl), but they go sideways (i.e. observations in rows, Python style).\n",
    "I think the real solution to a good interface does need to be thinking more like (or using) MLDataUtils.jl, which is Observation dimention agnostic, defaulting to normal julia practice (`ObsDim.Last()`).\n",
    "\n",
    "Using these we can define our metrics, etc.\n",
    "It might be nicer to be using [MLMetrics.jl](https://github.com/JuliaML/MLMetrics.jl/) to do this for us.\n",
    "But I'll just do it simply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import StatsBase: fit!, predict\n",
    "\n",
    "classify(model, features) = predict(model, features).>0.5\n",
    "accuracy(model, features, ground_truth_labels) = mean(classify(model, features) .== ground_truth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function\n",
    "Given a common interface we can write one function to evaluate them all.  \n",
    "Accessing our training and test data as a global variable.\n",
    "(Obviously not a good idea normally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent(x) = @sprintf(\"%0.2f%%\", 100*x)\n",
    "\n",
    "function evaluate(modeltype)    \n",
    "    @time model = fit!(modeltype, train_features, train_labels)\n",
    "    \n",
    "    println(\"$modeltype Train accuracy: \", percent(accuracy(model, train_features, train_labels)))\n",
    "    println(\"$modeltype Test accuracy: \", percent(accuracy(model, test_features, test_labels)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LIBLINEAR.jl](https://github.com/innerlee/LIBLINEAR.jl)\n",
    "\n",
    "Linear SVM.\n",
    "A great first shot for classification.\n",
    "It surprises me that the C backend was only created in 2008.\n",
    "\n",
    "R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.\n",
    "LIBLINEAR: A Library for Large Linear Classification, Journal of\n",
    "Machine Learning Research 9(2008), 1871-1874. Software available at\n",
    "http://www.csie.ntu.edu.tw/~cjlin/liblinear\n",
    "\n",
    "\n",
    "Because we are interesting in getting probabilities back from `predict`\n",
    "we are restricted to using `L2R_LR` and `L1R_LR` solver types, which are logistric regression.\n",
    "This could probably be relaxed for most applications (but might break those metrics defintions above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LIBLINEAR\n",
    "function fit!(::Type{LinearModel}, features, labels; solver_type=LIBLINEAR.L2R_LR, kwargs...)\n",
    "    linear_train(labels, features; solver_type=solver_type, kwargs...)\n",
    "end\n",
    "\n",
    "function predict(model::LinearModel, features)\n",
    "    classes, probs = linear_predict(model, features; probability_estimates=true)\n",
    "    vec(probs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.576060 seconds (31.21 k allocations: 1.764 MiB)\n",
      "LIBLINEAR.LinearModel Train accuracy: 19.57%\n",
      "LIBLINEAR.LinearModel Test accuracy: 19.86%\n"
     ]
    }
   ],
   "source": [
    "evaluate(LinearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl)\n",
    "\n",
    "The more general SVM package.\n",
    "We're here for its kernal SVM classifers.\n",
    "Again I am surprised that the backend was created so recently: 2005\n",
    "\n",
    "Since version 2.8, it implements an SMO-type algorithm proposed in this paper:\n",
    "R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training SVM. Journal of Machine Learning Research 6, 1889-1918, 2005.\n",
    "https://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "\n",
    "We're looking at `SVC`, in this example.\n",
    "The other types of interst here would be `NuSVC`, and `LinearSVC` (but we got that covered by LIBLINEAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 3 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import LIBSVM:svmtrain, SVM, svmpredict\n",
    "function fit!(::Type{SVM{Bool}}, features, labels; solver_type=LIBLINEAR.L2R_LR, kwargs...)\n",
    "    #could use ScikitLearnBase.fit!(SVC, features, Float64.(labels)), but it doesn't take extra args same way.\n",
    "    svmtrain(features, labels; probability=true, kwargs...)\n",
    "end\n",
    "\n",
    "function predict(model::SVM{Bool}, features)\n",
    "    classes, probs = svmpredict(model, features)\n",
    "    probs[1,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19.697385 seconds (131.22 k allocations: 7.247 MiB)\n",
      "LIBSVM.SVM{Bool} Train accuracy: 0.02%\n",
      "LIBSVM.SVM{Bool} Test accuracy: "
     ]
    }
   ],
   "source": [
    "evaluate(SVM{Bool})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl)\n",
    "Shallow decision trees are one of the only ML systems, that are not black box.\n",
    "Anyone can look in and see how they work.\n",
    "And while, the human domain expert might not make those rules themselves, they can say that \"yes those rules make sense\".\n",
    "This makes DecisionTrees way easier to \"sell\" to a consumer.\n",
    "\n",
    "This package has pruned trees, random forests and adaptive-boosted decision stumps.  \n",
    "They are all useful; and all suitable.  I've not evaluating the `AdaBoostStumpClassifier` as when I tried it threw errors.\n",
    "I'm going to just wrap the ScikitLearnBase interface here, as DecisionTree.jl seems to be alwary sideway anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using DecisionTree.predict in module Main conflicts with an existing identifier.\n",
      "WARNING: using DecisionTree.fit! in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fit! (generic function with 5 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DecisionTree\n",
    "import ScikitLearnBase\n",
    "\n",
    "function fit!(model::ScikitLearnBase.BaseClassifier, features, labels)\n",
    "    #could use ScikitLearnBase.fit!(SVC, features, Float64.(labels)), but it doesn't take extra args same way.\n",
    "    ScikitLearnBase.fit!(model, features', labels)\n",
    "end\n",
    "\n",
    "function predict(model::ScikitLearnBase.BaseClassifier, features)\n",
    "    ScikitLearnBase.predict_proba(model, features')[:,2]\n",
    "end\n",
    "\n",
    "#Use default settings if just give type\n",
    "function fit!(kind::Type{<:ScikitLearnBase.BaseClassifier}, features, labels)\n",
    "    fit!(kind(), features, labels)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.066056 seconds (1.76 M allocations: 126.120 MiB, 5.00% gc time)\n",
      "DecisionTree.RandomForestClassifier Train accuracy: 99.51%\n",
      "DecisionTree.RandomForestClassifier Test accuracy: 97.62%\n"
     ]
    }
   ],
   "source": [
    "evaluate(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.080683 seconds (39.47 k allocations: 6.837 MiB)\n",
      "DecisionTree.DecisionTreeClassifier Train accuracy: 100.00%\n",
      "DecisionTree.DecisionTreeClassifier Test accuracy: 97.29%\n"
     ]
    }
   ],
   "source": [
    "evaluate(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [XGBoost.jl](https://github.com/dmlc/XGBoost.jl)\n",
    "eXtreme Gradient Boosting.  \n",
    "These are boosting tree ensembles.\n",
    "They are all about adding extra rules to just handling the errors existing rules don't catch.\n",
    "This basically makes training fast, and simply improves the results.\n",
    "\n",
    "\n",
    "They win Kraggle all the time. Though today they lose the the plain old random forest.\n",
    "\n",
    "Right now, it hasn't had a release tagged, so is broken on julia 0.6\n",
    "This can be solved using `Pkg.checkout(\"XGBoost\"); Pkg.build(\"XGBoost\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import XGBoost: xgboost, Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 5 methods)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit!(::Type{Booster}, features, labels; num_rounds=16, eta=1, max_depth = 16, kwargs...)\n",
    "    xgboost(features', num_rounds; label=labels, objective = \"binary:logistic\",  eta=eta, max_depth=max_depth, silent=true, kwargs...)\n",
    "end\n",
    "\n",
    "function predict(model::Booster, features)\n",
    "    XGBoost.predict(model, features')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.442814 seconds (3.29 k allocations: 348.875 KiB)\n",
      "XGBoost.Booster Train accuracy: 99.98%\n",
      "XGBoost.Booster Test accuracy: 97.62%\n"
     ]
    }
   ],
   "source": [
    "evaluate(Booster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow.jl](https://github.com/malmaud/TensorFlow.jl)\n",
    "TensorFlow.jl is my baby: Ok it is actually [@malmaud's](http://www.malmaud.com/) baby.\n",
    "I guess I'm that cool uncle :-P that teach's it how to make slingshots and things; or in this case do julia-style indexing and such.\n",
    "\n",
    "Using a neural network to do classification is a bit like driving a tank to the shops.\n",
    "Sure it will get you there; and nothing can ever stop you getting there. But it's not exactly fast.\n",
    "So maybe you stop and give up before reaching the store.\n",
    "\n",
    "Since it is a general Neural Net framework (and a low level one at that), we have to build the classifier ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mRecompiling stale cache file /home/wheel/oxinabox/.julia/lib/v0.6/TensorFlow.ji for module TensorFlow.\n",
      "\u001b[39m\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mLoading a new version of TensorFlow.jl for the first time. This initial load can take around 5 minutes as code is precompiled; subsequent usage will only take a few seconds.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using TensorFlow\n",
    "\n",
    "struct TensorFlowClassifier\n",
    "    xs::Tensor{Float32}\n",
    "    ys::Tensor{Float32}\n",
    "    sess::Session\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-14 17:05:39.966669: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-14 17:05:39.966698: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Tensor loss/Add_5:1 shape=(1, ?) dtype=Float32>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = Session()\n",
    "\n",
    "nfeatures = 4\n",
    "@tf begin\n",
    "    xs = placeholder(Float32, shape=[nfeatures, -1]) #Nfeatures, Nsamples (going to use julian ordering)\n",
    "    z1 = xs\n",
    "    \n",
    "    W1 = get_variable([nfeatures^2, nfeatures], Float32)\n",
    "    b1 = get_variable([nfeatures^2], Float32)\n",
    "    z2 = nn.relu(W1*z1+b1)\n",
    "    \n",
    "    W2 = get_variable([nfeatures^2, nfeatures^2], Float32)\n",
    "    b2 = get_variable([nfeatures^2], Float32)\n",
    "    z3 =  nn.relu(W2*z2+b2)\n",
    "    \n",
    "    W3 = get_variable([1, nfeatures^2], Float32)\n",
    "    b3 = get_variable([nfeatures^2], Float32)\n",
    "    ys = nn.sigmoid(W3*z3+b3)\n",
    "\n",
    "    ys_targets = placeholder(Float32, shape=[1, -1])\n",
    "    loss = nn.sigmoid_cross_entropy_with_logits(;logits=ys, targets=ys_targets)\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: relu not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: relu not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`TensorFlow.nn.sigmoid_cross_entropy_with_logits` is a `Function`.\n",
       "\n",
       "```\n",
       "# 2 methods for generic function \"sigmoid_cross_entropy_with_logits\":\n",
       "sigmoid_cross_entropy_with_logits(; logits, targets, name) in TensorFlow.nn at /home/wheel/oxinabox/.julia/v0.6/TensorFlow/src/ops/nn.jl:195\n",
       "sigmoid_cross_entropy_with_logits(logits, targets; kwargs...) in TensorFlow.nn at deprecated.jl:56\n",
       "```\n"
      ],
      "text/plain": [
       "No documentation found.\n",
       "\n",
       "`TensorFlow.nn.sigmoid_cross_entropy_with_logits` is a `Function`.\n",
       "\n",
       "```\n",
       "# 2 methods for generic function \"sigmoid_cross_entropy_with_logits\":\n",
       "sigmoid_cross_entropy_with_logits(; logits, targets, name) in TensorFlow.nn at /home/wheel/oxinabox/.julia/v0.6/TensorFlow/src/ops/nn.jl:195\n",
       "sigmoid_cross_entropy_with_logits(logits, targets; kwargs...) in TensorFlow.nn at deprecated.jl:56\n",
       "```\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nn.sigmoid_cross_entropy_with_logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
